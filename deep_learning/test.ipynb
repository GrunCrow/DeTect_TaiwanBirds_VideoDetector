{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11b59e9",
   "metadata": {},
   "source": [
    "# YOLO Model Testing and Analysis Notebook\n",
    "This notebook tests the latest trained YOLO models on the test dataset with comprehensive confidence score analysis, prediction filtering, and detailed visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ded33",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8662efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a3c4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up matplotlib defaults\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb4031",
   "metadata": {},
   "source": [
    "## 2. Load Trained Model and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS_DIR = Path(\"DeTect-BMMS/runs\")\n",
    "DATASET_PATH = Path(\"../dataset/csvs/splits\")\n",
    "TEST_FILE = DATASET_PATH / \"test.txt\"\n",
    "\n",
    "# Class names from DeTect dataset\n",
    "CLASS_NAMES = {\n",
    "    0: 'bat',\n",
    "    1: 'bird',\n",
    "    2: 'insect',\n",
    "    3: 'drone',\n",
    "    4: 'plane',\n",
    "    5: 'other',\n",
    "    6: 'unknown'\n",
    "}\n",
    "\n",
    "# Find the latest trained model\n",
    "def find_latest_model(models_dir):\n",
    "    \"\"\"Find the most recent best.pt model\"\"\"\n",
    "    model_paths = list(models_dir.glob(\"*/weights/best.pt\"))\n",
    "    if not model_paths:\n",
    "        print(\"No trained models found!\")\n",
    "        return None\n",
    "    \n",
    "    # Sort by modification time\n",
    "    latest_model = max(model_paths, key=lambda p: p.stat().st_mtime)\n",
    "    return latest_model\n",
    "\n",
    "latest_model_path = find_latest_model(MODELS_DIR)\n",
    "print(f\"Latest model found: {latest_model_path}\")\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(str(latest_model_path))\n",
    "print(f\"Model loaded successfully!\")\n",
    "\n",
    "# Load test image paths\n",
    "with open(TEST_FILE, 'r') as f:\n",
    "    test_images = [Path(line.strip()) for line in f.readlines() if line.strip()]\n",
    "\n",
    "print(f\"Test dataset loaded: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f74cc",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a971df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(image_path):\n",
    "    \"\"\"Load YOLO format annotations from .txt file\"\"\"\n",
    "    txt_path = image_path.with_suffix('.txt')\n",
    "    \n",
    "    if not txt_path.exists():\n",
    "        return None\n",
    "    \n",
    "    annotations = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                class_id = int(parts[0])\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "                annotations.append({\n",
    "                    'class_id': class_id,\n",
    "                    'class_name': CLASS_NAMES.get(class_id, 'unknown'),\n",
    "                    'x_center': x_center,\n",
    "                    'y_center': y_center,\n",
    "                    'width': width,\n",
    "                    'height': height\n",
    "                })\n",
    "    \n",
    "    return annotations if annotations else None\n",
    "\n",
    "# Run inference on test dataset\n",
    "predictions_data = []\n",
    "\n",
    "print(f\"Running inference on {len(test_images)} test images...\")\n",
    "\n",
    "for idx, img_path in enumerate(test_images):\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_images)} images\")\n",
    "    \n",
    "    # Run prediction\n",
    "    results = model.predict(source=str(img_path), conf=0.1, verbose=False)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Load ground truth annotations\n",
    "    gt_annotations = load_annotations(img_path)\n",
    "    \n",
    "    # Extract predictions\n",
    "    if result.boxes is not None and len(result.boxes) > 0:\n",
    "        for box in result.boxes:\n",
    "            pred_class_id = int(box.cls)\n",
    "            confidence = float(box.conf)\n",
    "            \n",
    "            predictions_data.append({\n",
    "                'image_path': str(img_path),\n",
    "                'image_name': img_path.name,\n",
    "                'pred_class_id': pred_class_id,\n",
    "                'pred_class_name': CLASS_NAMES.get(pred_class_id, 'unknown'),\n",
    "                'confidence': confidence,\n",
    "                'gt_annotations': gt_annotations\n",
    "            })\n",
    "    else:\n",
    "        # Image with no detections\n",
    "        predictions_data.append({\n",
    "            'image_path': str(img_path),\n",
    "            'image_name': img_path.name,\n",
    "            'pred_class_id': -1,\n",
    "            'pred_class_name': 'no_detection',\n",
    "            'confidence': 0.0,\n",
    "            'gt_annotations': gt_annotations\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_data)\n",
    "print(f\"\\nInference complete! Generated {len(predictions_df)} predictions\")\n",
    "print(f\"DataFrame shape: {predictions_df.shape}\")\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce88bac",
   "metadata": {},
   "source": [
    "## 4. Calculate Confidence Scores and Extract Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth class (take the first annotation if multiple exist)\n",
    "def extract_ground_truth(annotations):\n",
    "    \"\"\"Extract ground truth class from annotations\"\"\"\n",
    "    if annotations is None or len(annotations) == 0:\n",
    "        return -1, 'no_annotation'\n",
    "    return annotations[0]['class_id'], annotations[0]['class_name']\n",
    "\n",
    "predictions_df[['gt_class_id', 'gt_class_name']] = predictions_df['gt_annotations'].apply(\n",
    "    lambda x: pd.Series(extract_ground_truth(x))\n",
    ")\n",
    "\n",
    "# Determine if prediction is correct\n",
    "predictions_df['is_correct'] = (predictions_df['pred_class_id'] == predictions_df['gt_class_id']) & \\\n",
    "                                (predictions_df['gt_class_id'] >= 0)\n",
    "\n",
    "# Confidence ranges\n",
    "def assign_confidence_range(conf):\n",
    "    if conf < 0.5:\n",
    "        return '<0.5'\n",
    "    elif conf < 0.7:\n",
    "        return '0.5-0.7'\n",
    "    elif conf < 0.9:\n",
    "        return '0.7-0.9'\n",
    "    else:\n",
    "        return '>0.9'\n",
    "\n",
    "predictions_df['confidence_range'] = predictions_df['confidence'].apply(assign_confidence_range)\n",
    "\n",
    "# Statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal predictions: {len(predictions_df)}\")\n",
    "print(f\"Correct predictions: {predictions_df['is_correct'].sum()}\")\n",
    "print(f\"Incorrect predictions: {(~predictions_df['is_correct']).sum()}\")\n",
    "print(f\"Accuracy: {predictions_df['is_correct'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\nConfidence score statistics:\")\n",
    "print(f\"  Mean: {predictions_df['confidence'].mean():.4f}\")\n",
    "print(f\"  Median: {predictions_df['confidence'].median():.4f}\")\n",
    "print(f\"  Min: {predictions_df['confidence'].min():.4f}\")\n",
    "print(f\"  Max: {predictions_df['confidence'].max():.4f}\")\n",
    "print(f\"  Std: {predictions_df['confidence'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nPredictions by confidence range:\")\n",
    "print(predictions_df['confidence_range'].value_counts().sort_index())\n",
    "\n",
    "predictions_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e70a2",
   "metadata": {},
   "source": [
    "## 5. Filter Predictions by Confidence Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44844adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_confidence(df, min_conf=0.0, max_conf=1.0):\n",
    "    \"\"\"Filter predictions by confidence threshold range\"\"\"\n",
    "    return df[(df['confidence'] >= min_conf) & (df['confidence'] < max_conf)].copy()\n",
    "\n",
    "# Interactive confidence filtering\n",
    "def get_filtered_stats(df, min_conf=0.0, max_conf=1.0):\n",
    "    \"\"\"Get statistics for filtered predictions\"\"\"\n",
    "    filtered = filter_by_confidence(df, min_conf, max_conf)\n",
    "    \n",
    "    if len(filtered) == 0:\n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        'count': len(filtered),\n",
    "        'accuracy': filtered['is_correct'].mean(),\n",
    "        'correct': filtered['is_correct'].sum(),\n",
    "        'incorrect': (~filtered['is_correct']).sum(),\n",
    "        'mean_confidence': filtered['confidence'].mean(),\n",
    "    }\n",
    "\n",
    "# Show filtering examples\n",
    "print(\"Filtering examples by confidence thresholds:\")\n",
    "print(\"\\nConf >= 0.5:\")\n",
    "print(get_filtered_stats(predictions_df, 0.5, 1.0))\n",
    "\n",
    "print(\"\\nConf >= 0.7:\")\n",
    "print(get_filtered_stats(predictions_df, 0.7, 1.0))\n",
    "\n",
    "print(\"\\nConf >= 0.9:\")\n",
    "print(get_filtered_stats(predictions_df, 0.9, 1.0))\n",
    "\n",
    "print(\"\\nConf < 0.5:\")\n",
    "print(get_filtered_stats(predictions_df, 0.0, 0.5))\n",
    "\n",
    "# Function to display predictions\n",
    "def display_predictions(df, title=\"\", n=10):\n",
    "    \"\"\"Display sample predictions\"\"\"\n",
    "    print(f\"\\n{title} (showing {min(n, len(df))} of {len(df)} records)\")\n",
    "    print(\"=\" * 120)\n",
    "    cols_to_show = ['image_name', 'pred_class_name', 'confidence', 'gt_class_name', 'is_correct']\n",
    "    print(df[cols_to_show].head(n).to_string(index=False))\n",
    "    print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de634ab4",
   "metadata": {},
   "source": [
    "## 6. Identify Correct and Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate correct and incorrect predictions\n",
    "correct_predictions = predictions_df[predictions_df['is_correct']].copy()\n",
    "incorrect_predictions = predictions_df[~predictions_df['is_correct']].copy()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION ACCURACY BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCorrect Predictions: {len(correct_predictions)} ({len(correct_predictions)/len(predictions_df)*100:.2f}%)\")\n",
    "print(f\"Incorrect Predictions: {len(incorrect_predictions)} ({len(incorrect_predictions)/len(predictions_df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCorrect predictions by confidence range:\")\n",
    "print(correct_predictions['confidence_range'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nIncorrect predictions by confidence range:\")\n",
    "print(incorrect_predictions['confidence_range'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nMean confidence for correct predictions: {correct_predictions['confidence'].mean():.4f}\")\n",
    "print(f\"Mean confidence for incorrect predictions: {incorrect_predictions['confidence'].mean():.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "display_predictions(correct_predictions.sort_values('confidence'), \n",
    "                   title=\"SAMPLE CORRECT PREDICTIONS (sorted by confidence)\", n=10)\n",
    "display_predictions(incorrect_predictions.sort_values('confidence'), \n",
    "                   title=\"SAMPLE INCORRECT PREDICTIONS (sorted by confidence)\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d55c",
   "metadata": {},
   "source": [
    "## 7. Plot Lowest Confidence Score Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def plot_image_with_info(ax, image_path, pred_class, pred_conf, gt_class, is_correct, title_suffix=\"\"):\n",
    "    \"\"\"Plot image with prediction information\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        ax.imshow(img)\n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f\"Error loading image:\\n{str(e)}\", \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # Create title with prediction info\n",
    "    title_color = 'green' if is_correct else 'red'\n",
    "    title = f\"Pred: {pred_class} ({pred_conf:.3f})\\nGT: {gt_class}\\n{title_suffix}\"\n",
    "    ax.set_title(title, fontsize=10, fontweight='bold', color=title_color)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Get lowest confidence predictions\n",
    "lowest_conf_predictions = predictions_df.nsmallest(9, 'confidence')\n",
    "\n",
    "print(f\"Visualizing {len(lowest_conf_predictions)} lowest confidence predictions...\")\n",
    "print(lowest_conf_predictions[['image_name', 'pred_class_name', 'confidence', 'gt_class_name', 'is_correct']].to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Lowest Confidence Score Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes.flat, lowest_conf_predictions.iterrows())):\n",
    "    image_path = row['image_path']\n",
    "    if os.path.exists(image_path):\n",
    "        plot_image_with_info(\n",
    "            ax, image_path,\n",
    "            row['pred_class_name'], row['confidence'],\n",
    "            row['gt_class_name'], row['is_correct'],\n",
    "            f\"Conf: {row['confidence']:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"Image not found:\\n{row['image_name']}\", \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lowest confidence predictions visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab31c4",
   "metadata": {},
   "source": [
    "## 8. Plot Highest Confidence Score Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest confidence predictions\n",
    "highest_conf_predictions = predictions_df.nlargest(9, 'confidence')\n",
    "\n",
    "print(f\"Visualizing {len(highest_conf_predictions)} highest confidence predictions...\")\n",
    "print(highest_conf_predictions[['image_name', 'pred_class_name', 'confidence', 'gt_class_name', 'is_correct']].to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Highest Confidence Score Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes.flat, highest_conf_predictions.iterrows())):\n",
    "    image_path = row['image_path']\n",
    "    if os.path.exists(image_path):\n",
    "        plot_image_with_info(\n",
    "            ax, image_path,\n",
    "            row['pred_class_name'], row['confidence'],\n",
    "            row['gt_class_name'], row['is_correct'],\n",
    "            f\"Conf: {row['confidence']:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"Image not found:\\n{row['image_name']}\", \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Highest confidence predictions visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac49eed",
   "metadata": {},
   "source": [
    "## 9. Plot Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample of incorrect predictions (sorted by confidence - highest first)\n",
    "sample_incorrect = incorrect_predictions.nlargest(9, 'confidence')\n",
    "\n",
    "print(f\"Visualizing {len(sample_incorrect)} incorrect predictions (highest confidence errors)...\")\n",
    "print(sample_incorrect[['image_name', 'pred_class_name', 'confidence', 'gt_class_name']].to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Incorrect Predictions (High Confidence Errors)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes.flat, sample_incorrect.iterrows())):\n",
    "    image_path = row['image_path']\n",
    "    if os.path.exists(image_path):\n",
    "        plot_image_with_info(\n",
    "            ax, image_path,\n",
    "            row['pred_class_name'], row['confidence'],\n",
    "            row['gt_class_name'], False,\n",
    "            f\"ERROR: Conf {row['confidence']:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"Image not found:\\n{row['image_name']}\", \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Incorrect predictions visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7af340",
   "metadata": {},
   "source": [
    "## 10. Plot Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6648948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample of correct predictions (sorted by confidence - lowest first to see uncertain correct ones)\n",
    "sample_correct_uncertain = correct_predictions.nsmallest(9, 'confidence')\n",
    "\n",
    "print(f\"Visualizing {len(sample_correct_uncertain)} correct predictions (uncertain but correct)...\")\n",
    "print(sample_correct_uncertain[['image_name', 'pred_class_name', 'confidence', 'gt_class_name']].to_string(index=False))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "fig.suptitle('Correct Predictions (Uncertain but Correct)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes.flat, sample_correct_uncertain.iterrows())):\n",
    "    image_path = row['image_path']\n",
    "    if os.path.exists(image_path):\n",
    "        plot_image_with_info(\n",
    "            ax, image_path,\n",
    "            row['pred_class_name'], row['confidence'],\n",
    "            row['gt_class_name'], True,\n",
    "            f\"CORRECT: Conf {row['confidence']:.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"Image not found:\\n{row['image_name']}\", \n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correct predictions visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349cad9",
   "metadata": {},
   "source": [
    "## 11. Visualize Ground Truth vs Predictions - Confidence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e65620",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Ground Truth vs Predictions Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Confidence distribution by correctness\n",
    "ax = axes[0, 0]\n",
    "correct_predictions['confidence'].hist(bins=30, alpha=0.6, label='Correct', ax=ax, color='green')\n",
    "incorrect_predictions['confidence'].hist(bins=30, alpha=0.6, label='Incorrect', ax=ax, color='red')\n",
    "ax.set_xlabel('Confidence Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Confidence Score Distribution: Correct vs Incorrect')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Predictions by confidence range\n",
    "ax = axes[0, 1]\n",
    "conf_range_counts = predictions_df['confidence_range'].value_counts().sort_index()\n",
    "colors = ['#ff6b6b', '#ffa500', '#4ecdc4', '#45b7d1']\n",
    "bars = ax.bar(range(len(conf_range_counts)), conf_range_counts.values, color=colors)\n",
    "ax.set_xticks(range(len(conf_range_counts)))\n",
    "ax.set_xticklabels(conf_range_counts.index, rotation=0)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Predictions by Confidence Range')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Accuracy by confidence range\n",
    "ax = axes[1, 0]\n",
    "accuracy_by_range = predictions_df.groupby('confidence_range')['is_correct'].agg(['sum', 'count'])\n",
    "accuracy_by_range['accuracy'] = accuracy_by_range['sum'] / accuracy_by_range['count']\n",
    "x_pos = range(len(accuracy_by_range))\n",
    "bars = ax.bar(x_pos, accuracy_by_range['accuracy'].values, color=colors)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(accuracy_by_range.index, rotation=0)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_title('Accuracy by Confidence Range')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=predictions_df['is_correct'].mean(), color='r', linestyle='--', label='Overall Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2%}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Confidence score statistics\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "CONFIDENCE STATISTICS\n",
    "\n",
    "Overall:\n",
    "  Total Predictions: {len(predictions_df)}\n",
    "  Mean Confidence: {predictions_df['confidence'].mean():.4f}\n",
    "  Median Confidence: {predictions_df['confidence'].median():.4f}\n",
    "  Std Dev: {predictions_df['confidence'].std():.4f}\n",
    "\n",
    "Correct Predictions:\n",
    "  Count: {len(correct_predictions)}\n",
    "  Mean Confidence: {correct_predictions['confidence'].mean():.4f}\n",
    "  Accuracy: {len(correct_predictions)/len(predictions_df):.2%}\n",
    "\n",
    "Incorrect Predictions:\n",
    "  Count: {len(incorrect_predictions)}\n",
    "  Mean Confidence: {incorrect_predictions['confidence'].mean():.4f}\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.9, stats_text, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541befb",
   "metadata": {},
   "source": [
    "## 12. Create Confidence Score Calibration Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for calibration (only valid predictions with ground truth)\n",
    "valid_predictions = predictions_df[predictions_df['gt_class_id'] >= 0].copy()\n",
    "\n",
    "if len(valid_predictions) > 0:\n",
    "    # Convert is_correct to binary (1 for correct, 0 for incorrect)\n",
    "    y_true = valid_predictions['is_correct'].astype(int).values\n",
    "    y_scores = valid_predictions['confidence'].values\n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_scores, n_bins=10, strategy='uniform')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle('Confidence Score Calibration Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Calibration curve\n",
    "    ax = axes[0]\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')\n",
    "    ax.plot(prob_pred, prob_true, 's-', label='Model', linewidth=2, markersize=8)\n",
    "    ax.set_xlabel('Mean Predicted Confidence', fontsize=12)\n",
    "    ax.set_ylabel('Empirical Probability', fontsize=12)\n",
    "    ax.set_title('Calibration Curve\\n(Expected Calibration Error)')\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # 2. Reliability diagram (histogram)\n",
    "    ax = axes[1]\n",
    "    n, bins, patches = ax.hist(y_scores, bins=10, range=(0, 1), alpha=0.3, \n",
    "                               edgecolor='black', label='Sample distribution')\n",
    "    \n",
    "    # Color bars based on bin accuracy\n",
    "    for i in range(len(prob_pred)):\n",
    "        bin_center = (bins[i] + bins[i+1]) / 2\n",
    "        patch_idx = i\n",
    "        if patch_idx < len(patches):\n",
    "            # Color based on calibration goodness\n",
    "            if abs(prob_pred[i] - prob_true[i]) < 0.1:\n",
    "                patches[patch_idx].set_facecolor('green')\n",
    "            elif abs(prob_pred[i] - prob_true[i]) < 0.2:\n",
    "                patches[patch_idx].set_facecolor('yellow')\n",
    "            else:\n",
    "                patches[patch_idx].set_facecolor('red')\n",
    "    \n",
    "    ax.plot(prob_pred, len(y_scores) * 0.1, 'rs-', label='Bin accuracy', markersize=8)\n",
    "    ax.set_xlabel('Confidence Score Bins', fontsize=12)\n",
    "    ax.set_ylabel('Count / Accuracy', fontsize=12)\n",
    "    ax.set_title('Reliability Diagram')\n",
    "    ax.legend(loc='upper left', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate calibration metrics\n",
    "    expected_calibration_error = np.mean(np.abs(prob_pred - prob_true))\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CALIBRATION METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Expected Calibration Error (ECE): {expected_calibration_error:.4f}\")\n",
    "    print(f\"\\nCalibration by bin:\")\n",
    "    print(f\"{'Bin':<15} {'Mean Pred':<15} {'Accuracy':<15} {'Difference':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (pred, true) in enumerate(zip(prob_pred, prob_true)):\n",
    "        print(f\"Bin {i+1:<10} {pred:<15.4f} {true:<15.4f} {abs(pred-true):<15.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions with ground truth available for calibration analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bbcde",
   "metadata": {},
   "source": [
    "## 13. Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606af6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix for valid predictions\n",
    "valid_preds = predictions_df[predictions_df['gt_class_id'] >= 0].copy()\n",
    "\n",
    "if len(valid_preds) > 0:\n",
    "    y_true = valid_preds['gt_class_id'].values\n",
    "    y_pred = valid_preds['pred_class_id'].values\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get unique classes\n",
    "    unique_classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    class_labels = [CLASS_NAMES.get(c, f'Class {c}') for c in unique_classes]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                xticklabels=class_labels, yticklabels=class_labels,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix - All Predictions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed classification report\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(classification_report(y_true, y_pred, target_names=class_labels, digits=4))\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OVERALL STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total valid predictions: {len(valid_preds)}\")\n",
    "    print(f\"Overall Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"Macro Precision: {precision_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "    print(f\"Macro Recall: {recall_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "    print(f\"Macro F1: {f1_score(y_true, y_pred, average='macro', zero_division=0):.4f}\")\n",
    "    print(f\"Weighted Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"Weighted Recall: {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"Weighted F1: {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions with ground truth for confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bb242",
   "metadata": {},
   "source": [
    "## 14. Calculate Performance Metrics by Confidence Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ebbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each confidence range\n",
    "confidence_ranges = ['<0.5', '0.5-0.7', '0.7-0.9', '>0.9']\n",
    "metrics_by_range = []\n",
    "\n",
    "for conf_range in confidence_ranges:\n",
    "    range_data = predictions_df[predictions_df['confidence_range'] == conf_range]\n",
    "    \n",
    "    if len(range_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Filter for valid predictions with ground truth\n",
    "    valid_range_data = range_data[range_data['gt_class_id'] >= 0]\n",
    "    \n",
    "    if len(valid_range_data) == 0:\n",
    "        metrics_by_range.append({\n",
    "            'confidence_range': conf_range,\n",
    "            'count': len(range_data),\n",
    "            'accuracy': np.nan,\n",
    "            'precision': np.nan,\n",
    "            'recall': np.nan,\n",
    "            'f1': np.nan,\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    y_true_range = valid_range_data['gt_class_id'].values\n",
    "    y_pred_range = valid_range_data['pred_class_id'].values\n",
    "    \n",
    "    metrics_by_range.append({\n",
    "        'confidence_range': conf_range,\n",
    "        'count': len(range_data),\n",
    "        'valid_count': len(valid_range_data),\n",
    "        'accuracy': accuracy_score(y_true_range, y_pred_range),\n",
    "        'precision': precision_score(y_true_range, y_pred_range, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true_range, y_pred_range, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(y_true_range, y_pred_range, average='weighted', zero_division=0),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_by_range)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Performance Metrics by Confidence Range', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy by range\n",
    "ax = axes[0, 0]\n",
    "ax.bar(metrics_df['confidence_range'], metrics_df['accuracy'], color='steelblue', alpha=0.7)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('Accuracy by Confidence Range')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(metrics_df['accuracy']):\n",
    "    if not np.isnan(v):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Precision by range\n",
    "ax = axes[0, 1]\n",
    "ax.bar(metrics_df['confidence_range'], metrics_df['precision'], color='seagreen', alpha=0.7)\n",
    "ax.set_ylabel('Precision', fontsize=11)\n",
    "ax.set_title('Precision by Confidence Range')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(metrics_df['precision']):\n",
    "    if not np.isnan(v):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Recall by range\n",
    "ax = axes[1, 0]\n",
    "ax.bar(metrics_df['confidence_range'], metrics_df['recall'], color='coral', alpha=0.7)\n",
    "ax.set_ylabel('Recall', fontsize=11)\n",
    "ax.set_title('Recall by Confidence Range')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(metrics_df['recall']):\n",
    "    if not np.isnan(v):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. F1-Score by range\n",
    "ax = axes[1, 1]\n",
    "ax.bar(metrics_df['confidence_range'], metrics_df['f1'], color='mediumpurple', alpha=0.7)\n",
    "ax.set_ylabel('F1-Score', fontsize=11)\n",
    "ax.set_title('F1-Score by Confidence Range')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(metrics_df['f1']):\n",
    "    if not np.isnan(v):\n",
    "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PERFORMANCE METRICS BY CONFIDENCE RANGE\")\n",
    "print(\"=\" * 100)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125e302",
   "metadata": {},
   "source": [
    "## 15. Interactive Prediction Filtering Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to easily filter and display predictions\n",
    "def filter_and_analyze(min_confidence=0.5, max_confidence=1.0, \n",
    "                       correct_only=False, incorrect_only=False, \n",
    "                       class_filter=None, show_count=10):\n",
    "    \"\"\"\n",
    "    Filter predictions and display statistics\n",
    "    \n",
    "    Parameters:\n",
    "    - min_confidence: minimum confidence threshold\n",
    "    - max_confidence: maximum confidence threshold\n",
    "    - correct_only: show only correct predictions\n",
    "    - incorrect_only: show only incorrect predictions\n",
    "    - class_filter: filter by predicted class name (e.g., 'bird', 'bat')\n",
    "    - show_count: number of samples to display\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with all predictions\n",
    "    filtered = predictions_df.copy()\n",
    "    \n",
    "    # Apply confidence filter\n",
    "    filtered = filtered[(filtered['confidence'] >= min_confidence) & \n",
    "                       (filtered['confidence'] < max_confidence)]\n",
    "    \n",
    "    # Apply correctness filter\n",
    "    if correct_only:\n",
    "        filtered = filtered[filtered['is_correct']]\n",
    "    if incorrect_only:\n",
    "        filtered = filtered[~filtered['is_correct']]\n",
    "    \n",
    "    # Apply class filter\n",
    "    if class_filter:\n",
    "        filtered = filtered[filtered['pred_class_name'] == class_filter.lower()]\n",
    "    \n",
    "    print(f\"\\nFiltered Results:\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    print(f\"Total matching predictions: {len(filtered)}\")\n",
    "    if len(filtered) > 0:\n",
    "        print(f\"Mean confidence: {filtered['confidence'].mean():.4f}\")\n",
    "        print(f\"Accuracy: {filtered['is_correct'].mean():.4f}\")\n",
    "        print(f\"\\nPrediction distribution:\")\n",
    "        print(filtered['pred_class_name'].value_counts())\n",
    "        print(f\"\\n{'=' * 100}\")\n",
    "        print(f\"Sample predictions (showing {min(show_count, len(filtered))} of {len(filtered)}):\")\n",
    "        cols = ['image_name', 'pred_class_name', 'confidence', 'gt_class_name', 'is_correct']\n",
    "        display_df = filtered[cols].sort_values('confidence', ascending=False).head(show_count)\n",
    "        print(display_df.to_string(index=False))\n",
    "    print(f\"{'=' * 100}\\n\")\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Example usage:\n",
    "print(\"Example 1: High confidence predictions (>0.9)\")\n",
    "example1 = filter_and_analyze(min_confidence=0.9)\n",
    "\n",
    "print(\"\\nExample 2: Uncertain but correct predictions (0.5-0.7, correct only)\")\n",
    "example2 = filter_and_analyze(min_confidence=0.5, max_confidence=0.7, correct_only=True)\n",
    "\n",
    "print(\"\\nExample 3: High confidence errors (>0.7, incorrect only)\")\n",
    "example3 = filter_and_analyze(min_confidence=0.7, incorrect_only=True, show_count=15)\n",
    "\n",
    "print(\"\\nExample 4: Bird predictions\")\n",
    "example4 = filter_and_analyze(class_filter='bird', show_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81420a1",
   "metadata": {},
   "source": [
    "## 16. Summary Report and Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\" \" * 30 + \"TEST RESULTS SUMMARY REPORT\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Model path: {latest_model_path}\")\n",
    "print(f\"Test dataset size: {len(test_images)} images\")\n",
    "print(f\"\\n\" + \"-\" * 100)\n",
    "\n",
    "print(\"\\nOVERALL PERFORMANCE METRICS\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "valid_preds_for_metrics = predictions_df[predictions_df['gt_class_id'] >= 0]\n",
    "if len(valid_preds_for_metrics) > 0:\n",
    "    y_true_all = valid_preds_for_metrics['gt_class_id'].values\n",
    "    y_pred_all = valid_preds_for_metrics['pred_class_id'].values\n",
    "    \n",
    "    print(f\"Total valid predictions: {len(valid_preds_for_metrics)}\")\n",
    "    print(f\"Overall Accuracy: {accuracy_score(y_true_all, y_pred_all):.4f}\")\n",
    "    print(f\"Precision (weighted): {precision_score(y_true_all, y_pred_all, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"Recall (weighted): {recall_score(y_true_all, y_pred_all, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"F1-Score (weighted): {f1_score(y_true_all, y_pred_all, average='weighted', zero_division=0):.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 100)\n",
    "print(\"\\nCONFIDENCE SCORE ANALYSIS\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Mean confidence: {predictions_df['confidence'].mean():.4f}\")\n",
    "print(f\"Median confidence: {predictions_df['confidence'].median():.4f}\")\n",
    "print(f\"Min confidence: {predictions_df['confidence'].min():.4f}\")\n",
    "print(f\"Max confidence: {predictions_df['confidence'].max():.4f}\")\n",
    "print(f\"Std deviation: {predictions_df['confidence'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nCorrect predictions mean confidence: {correct_predictions['confidence'].mean():.4f}\")\n",
    "print(f\"Incorrect predictions mean confidence: {incorrect_predictions['confidence'].mean():.4f}\")\n",
    "print(f\"Confidence difference: {correct_predictions['confidence'].mean() - incorrect_predictions['confidence'].mean():.4f}\")\n",
    "\n",
    "if len(valid_predictions) > 0:\n",
    "    print(f\"\\nCalibration Error (ECE): {expected_calibration_error:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 100)\n",
    "print(\"\\nCLASS-WISE PERFORMANCE\")\n",
    "print(\"-\" * 100)\n",
    "if len(valid_preds_for_metrics) > 0:\n",
    "    for class_id, class_name in sorted(CLASS_NAMES.items()):\n",
    "        mask = y_true_all == class_id\n",
    "        if mask.sum() > 0:\n",
    "            class_accuracy = accuracy_score(y_true_all[mask], y_pred_all[mask])\n",
    "            class_count = mask.sum()\n",
    "            print(f\"{class_name:<15}: {class_count:>4} samples, Accuracy: {class_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Export predictions to CSV\n",
    "output_file = Path(\"test_predictions.csv\")\n",
    "predictions_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nPredictions exported to: {output_file}\")\n",
    "\n",
    "# Export summary statistics\n",
    "summary_file = Path(\"test_summary.txt\")\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"=\" * 100 + \"\\n\")\n",
    "    f.write(\" \" * 30 + \"TEST RESULTS SUMMARY\\n\")\n",
    "    f.write(\"=\" * 100 + \"\\n\")\n",
    "    f.write(f\"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model path: {latest_model_path}\\n\")\n",
    "    f.write(f\"Test dataset: {len(test_images)} images\\n\")\n",
    "    f.write(f\"\\nOverall Accuracy: {accuracy_score(y_true_all, y_pred_all):.4f}\\n\")\n",
    "    f.write(f\"Precision (weighted): {precision_score(y_true_all, y_pred_all, average='weighted', zero_division=0):.4f}\\n\")\n",
    "    f.write(f\"Recall (weighted): {recall_score(y_true_all, y_pred_all, average='weighted', zero_division=0):.4f}\\n\")\n",
    "    f.write(f\"F1-Score (weighted): {f1_score(y_true_all, y_pred_all, average='weighted', zero_division=0):.4f}\\n\")\n",
    "\n",
    "print(f\"Summary exported to: {summary_file}\")\n",
    "print(\"\\nâœ“ Test analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualizations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
